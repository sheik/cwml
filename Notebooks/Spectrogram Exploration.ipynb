{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sample_rate, samples = wav.read(\"data/arrl-example-18wpm.wav\")\n",
    "f, t, Zxx = signal.stft(samples, fs=8000)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.ylim(400, 1000)\n",
    "_ = plt.pcolormesh(t, f, np.abs(Zxx), shading='auto', vmax=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate, samples = wav.read(\"data/generated-example.wav\")\n",
    "f, t, Zxx = signal.stft(samples, fs=8000)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.ylim(0, 1000)\n",
    "_ = plt.pcolormesh(t, f, np.abs(Zxx), shading='auto', vmax=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from matplotlib.mlab import specgram\n",
    "nfft = 256\n",
    "overlap = nfft - 56  # overlap value for spectrogram\n",
    "\n",
    "def get_specgram(signal, rate):\n",
    "    arr2D, freqs, bins = specgram(\n",
    "        signal,\n",
    "        window=np.blackman(nfft),\n",
    "        Fs=rate,\n",
    "        NFFT=nfft,\n",
    "        noverlap=overlap,\n",
    "        pad_to=32 * nfft,\n",
    "    )\n",
    "    return arr2D, freqs, bins\n",
    "\n",
    "def plot_image(arr2D, bins, freqs):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    extent = (bins[0], bins[-1], freqs[-1], freqs[0])\n",
    "    im = ax.imshow(\n",
    "        arr2D,\n",
    "        aspect=\"auto\",\n",
    "        extent=extent,\n",
    "        interpolation=\"none\",\n",
    "        cmap=\"Greys\",\n",
    "        norm=None,\n",
    "    )\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "def normalize_image(img):\n",
    "    # normalize\n",
    "    (m, s) = cv2.meanStdDev(img)\n",
    "    m = m[0][0]\n",
    "    s = s[0][0]\n",
    "    img = img - m\n",
    "    img = img / s if s>0 else img\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_image(filename):\n",
    "    imgSize=(256, 32)\n",
    "    dataAugmentation=False\n",
    "\n",
    "    imgname = filename+\".png\"   \n",
    "\n",
    "    # Load  image in grayscale if exists\n",
    "    img = cv2.imread(imgname, 0) \n",
    "\n",
    "    # TODO: re-enable this IF statement\n",
    "    #if img is None:\n",
    "    rate, data = wavfile.read(filename)\n",
    "    arr2D, freqs, bins = get_specgram(data, rate)\n",
    "\n",
    "    # Get the image data array shape (Freq bins, Time Steps)\n",
    "    shape = arr2D.shape\n",
    "\n",
    "    # Find the CW spectrum peak - look across all time steps\n",
    "    f = int(np.argmax(arr2D[:]) / shape[1])\n",
    "\n",
    "    time_steps = (4.0/(len(data)/rate))*shape[1]\n",
    "\n",
    "    # Create a 32x128 array centered to spectrum peak\n",
    "    img = cv2.resize(arr2D[f - 16 : f + 16][:], imgSize)\n",
    "\n",
    "    img = normalize_image(img)\n",
    "\n",
    "    cv2.imwrite(imgname, img*256.)\n",
    "\n",
    "    img = normalize_image(img)  \n",
    "    # transpose for TF\n",
    "    img = cv2.transpose(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_image('data/C.wav')\n",
    "from IPython.display import Image\n",
    "Image(filename='data/C.wav.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Padding for files with less than 256000 samples\n",
    "  #print(\"Len: {}\".format(tf.shape(waveform)))\n",
    "  zero_padding = tf.zeros([100000] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "  # Concatenate audio with padding so that all audio clips will be of the \n",
    "  # same length\n",
    "  waveform = tf.cast(waveform, tf.float32)\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "  #spectrogram = tf.expand_dims(equal_length, axis=0)\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "  return tf.squeeze(audio, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_spectrogram(decode_audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in s.numpy():\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
